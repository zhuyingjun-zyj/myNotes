#  <div align='center' ><font size='70'> ELECTRA详解</font></div>

### 概念

ELECTRA 的全称是：Efficiently Learning an Encoder that Classifies Token Replacements Accurately。是这几年一个比较创新的模型，从模型架构和预训练任务都和BERT有一定程度的不同。目前的SOTA语言表示学习方法可以看做是在学习一个**去噪自编码器**（denoising autoencoder），也就是**Masked Language Modeling（MLM）**的方法，它们选择无标注的输入序列的一小部分（通常15%），然后将这一部分mask掉（比如BERT），或者attend到这些token（比如XLNet），然后训练整个网络来还原原来的输入。但是electra其提出了Replaced Token Detection(RTD)，**字符替换探测**的训练方式。


### 模型结构

ELECTRA最大的贡献是提出了新的预训练任务和框架，在上述简介中也提到了。将生成式的MLM预训练任务改成了判别式的RTD任务，再判断当前token是否被替换过。具体流程如下：
使用一个MLM的G-BERT来对输入句子进行改造，然后丢给D-BERT去判断哪个字被修改过，如下：
<div align='center' ><img width="800" alt="1852906-20220623161931007-1534771153" src="https://user-images.githubusercontent.com/66345340/192463750-f668c905-7d93-4363-913d-8ee0e2a412c3.png"></div>

+  首先对一句话里面的token进行**随机的【MASK】**，然后训练一个生成器，对【MASK】掉的token进行预测（就是原生BERT的MLM任务），通常**生成器不需要很大**（原因在后面的实验部分有论证），生成器对【MASK】掉的token预测完后，**得到一句新的话**；

+  **然后输入判别器，判别器判断每个token，是否是原样本的，还是被替换过的**。注意的是，假如生成器预测出的token为原来的token，那这个token在判别器的输出标签里还是算原样本，而不是被替换过的（如上图的"the"，生成器预测为"the"，则"the"在判别器中的真实标签就为original，而不是replaced)。

#### **Replaced Token Detection**

Replaced Token Detection(RTD)，**字符替换探测**，是一种更有效的预训练任务。RTD方法不是掩盖输入，而是通过使用**生成网络来生成一些合理替换字符来达到破坏输入**的目的。然后，我们训练一个判别器模型，该模型可以预测当前字符是否被语言模型替换过。实验结果表明，**这种新的预训练任务比MLM更有效**，因为该任务是定义在所有文本输入上，而不是仅仅被掩盖的一小部分，在模型大小，数据和计算力相同的情况下，RTD方法所学习的上下文表示远远优于BERT所学习的上下文表示。

但RTD结构有个问题，输入句子经过生成器，输出改写过的句子，因为句子的字词是离散的，所以梯度无法反向传播，判别器的梯度无法传给生成器，于是**生成器的目标还是MLM**，判别器的目标是**序列标注（判断每个字符是真是假)**，**两者同时训练，但是判别器的梯度不会传给生成器**，目标函数如下：
<img width="1000" alt="IMG20220927154955" src="https://user-images.githubusercontent.com/66345340/192466239-d63bc2f3-960f-44de-b674-22dec3d93fec.png">

因为判别器的任务相对来说简单些，RTD损失相对MLM损失会很小，因此加上一个系数，论文中使用了50。

经过预训练，在下游任务的使用中，**直接给出生成器，在判别器进行微调**。

另外，在优化判别器时计算了所有token上的损失，而以往计算BERT的MLM loss时会忽略没被mask的token。作者在后来的实验中也验证了在所有token上进行损失计算会提升效率(训练效率)和效果。

#### **权重共享**

+ 假如生成器和判别器采用同样架构的话，则两个模型可以权重共享，
+ 假如不是同样架构的话，也可以共享embedding层。

所以作者分别对以下三种情况做了实验：

+ 生成器和判别器的参数独立，完全不共享；
+ 生成器和判别器的embedding参数共享，而且生成器 input层 和 output层 的embedding参数共享（想想为什么可以这样？因为生成器最后是一个全词表的分类，所以分类层跟输入时embedding矩阵的维度一致，而判别器最后是一个二分类，所以分类层跟embedding矩阵维度不一致），其他参数不共享；
+ 生成器和判别器的参数共享。

第一种方案GLUE score为83.6，第二种方案GLUE score为84.3，第三种方案GLUE score为84.4，从结果上，首先肯定的是共享参数能带来效果的提升，作者给出的理由是，假如不共享参数，**判别器只会对输入token的embedding进行更新，而生成器则会对全词表进行权重更新**（这里有疑惑的可以想想，生成器最后由于存在softmax层可是做了一个全词表的分类哦），**所以共享参数肯定是必要的**，至于为什么作者最后采用方案二是不是方案三呢，是因为假如采用方案三的话，**限定了生成器和判别器的模型结构要一样，极大影响了训练的效率。**

#### 更小的生成器
从权重共享的实验中看到，生成器和判别器只需要共享embedding 的权重就足够了。那这样的话是否可以缩小生成器的尺寸进行训练效率的提升呢？作者在保持原有的hidden size的设置下减少了层数，得到了下图所示的关系图：
<div align='center' ><img width="800" alt="1852906-20220623161956793-391215432" src="https://user-images.githubusercontent.com/66345340/192470606-3a16d347-5b4b-4401-9e53-bc28c0caf221.png"></div>

可以看到，**生成器的大小在判别器的1/4到1/2之间的效果是最好的**。作者认为原因是**过强的生成器会增加**判别器的难度。

#### 训练策略

+ 生成器和判别器联合训练；
+ Two-stage training：即先训练生成器，然后freeze掉，用生成器的权重初始化判别器，再接着训练相同步数的判别器。
+ Adversarial Contrastive Estimation：ELECTRA因为上述一些问题无法使用GAN，但也可以以一种对抗学习的思想来训练。作者将生成器的目标函数由最小化MLM损失换成了最大化判别器在被替换token上RTD损失。但还有一个问题，就是新的生成器无法用梯度上升更新生成器，于是作者使用强化学习Policy Gradient思想，最终优化下来生成器在MLM 任务上可以达到54%的准确率，而之前MLE优化下可达到65%。

这里先探讨策略一和策略二，作者发现，假如不采用权重共享的话，二步训练法训练完的判别器可能什么都学不到，作者提出的理由是判别器的学习晚生成器太多。除此之外，假如是参数共享，且采用二步法，在生成器刚学完，切换到判别器开始学的时候，GLUE score有一个显著的提升，理由可能是由于参数共享，所以判别器并不是从一个小白开始训练起。
<div align='center' ><img width="800" alt="1852906-20220623162014520-2003972442" src="https://user-images.githubusercontent.com/66345340/192493787-14ed0596-4905-437d-9b55-27344435df4b.png"></div>

然并卵，实验发现联合训练在GLUE上的指标是最好的，所以最后也是采用联合训练方法。而两段式的训练弱一些，作者猜测是生成器太强了导致判别任务难度增大。不过两段式最终效果也比BERT本身要强，进一步证明了判别式预训练的效果。

### 损失函数

 ELECTRA一共需要训练两个模型，一个是Generator G，另一个是Discriminator D，G和D都有一个Transformer的Encoder结构
 + **Generator** 事实上就是在执行MLM,MLM选择一些随机的位置（从1到n之间）来进行mask,具体的会再次采样将masked的替换,对于每个masked的位置词使用softmax计算，输出每个词的概率；
 + **Discriminator** 会识别输入的每个token是否为原来的token。其实就是做个二分类。

 整体的损失函数为：

<div align='center' ><img width="800" alt="微信图片_20220928173936" src="https://user-images.githubusercontent.com/66345340/192745934-d693cc20-aa3c-4f89-aad3-f12c1a2fddd5.png"></div>

这里的L是目标函数，E指期望值。如果你对GAN比较熟悉的话会发现这个损失函数很像GAN的损失函数，但是事实上和GAN还是有一定区别的：
+ 如果Generator恰好生成了原来的token，那么这个位置将不会认为是fake的；
+ Generator使用**极大似然**的方法来训练而不是使用对抗式的方法视图骗过Discriminator来训练的，对抗学习的方法是困难的因为不能通过Generator的采样进行反向传播。

### 对比、优缺点
ELECTRA效果之所以那么好，
* 大部分归功于对所有的token进行学习，
* 其次是由于缓解了预训练和fine tuning时输入不一致的问题。

### 常见的面试问题
 1、